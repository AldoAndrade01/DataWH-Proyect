# ğŸµ Data Warehouse de MÃºsica  
**Proyecto de la materia: Almacenes de Datos â€“ 7mo semestre**
**Equipo**

**Andrade Ramos Aldo Alberto - 217431633**
**Cano Lopez Brayan Oswaldo - 219423212 **
**Sekaran Rojo Mahatma - 218869276 **

---

## ğŸ§  DescripciÃ³n general

Este proyecto implementa un **pipeline ETL modular** (ExtracciÃ³n, TransformaciÃ³n y Carga) con **Pandas** y una **API REST** con **FastAPI**, que permite procesar, limpiar y consolidar datos musicales de Spotify provenientes de tres fuentes distintas.

El resultado final es un **Data Warehouse musical** que genera un dataset limpio y unificado (`tracks_clean.csv`) listo para anÃ¡lisis.

---

## ğŸ“ Estructura del proyecto

```

DataWH-Proyect/
â”œâ”€ data/
â”‚  â”œâ”€ raw/           # Archivos originales (D1, D2, D3)
â”‚  â”œâ”€ interim/       # Datos limpios por fuente (ETL intermedio)
â”‚  â””â”€ processed/     # Dataset final consolidado
â”œâ”€ eda/              # AnÃ¡lisis exploratorio de datos
â”‚  â””â”€ eda.ipynb
â”œâ”€ etl/              # CÃ³digo del proceso ETL (modular)
â”‚  â”œâ”€ extract.py
â”‚  â”œâ”€ transform_d1.py
â”‚  â”œâ”€ transform_d2.py
â”‚  â”œâ”€ transform_d3.py
â”‚  â”œâ”€ merge.py
â”‚  â”œâ”€ load.py
â”‚  â”œâ”€ pipeline.py
â”‚  â””â”€ configs.py
â”œâ”€ api/              # API REST con FastAPI
â”‚  â””â”€ main.py
â”œâ”€ docs/             # DocumentaciÃ³n
â”‚  â”œâ”€ arquitectura.md
â”‚  â”œâ”€ decisiones_eda.md
â”‚  â””â”€ bitacora_etl.md
â”œâ”€ tests/
â”‚  â””â”€ run_etl_local.py
â”œâ”€ .venv/            # Entorno virtual (no se sube a Git)
â”œâ”€ .gitignore
â”œâ”€ requirements.txt
â””â”€ README.md

````

---

## âš™ï¸ InstalaciÃ³n paso a paso

### 1ï¸âƒ£ Clonar el repositorio
```bash
git clone https://github.com/AldoAndrade01/DataWH-Proyect
cd DataWH-Proyect
````

### 2ï¸âƒ£ Crear entorno virtual

#### En **Windows PowerShell**

```powershell
python -m venv .venv
.\.venv\Scripts\Activate
```

### 3ï¸âƒ£ Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Revisar carpetas de datos

Confirma que existan:

```
data/raw/         â†’ aquÃ­ van los datasets originales (D1, D2, D3)
data/interim/     â†’ se generan automÃ¡ticamente (limpios)
data/processed/   â†’ se genera automÃ¡ticamente (final consolidado)
```

---

## ğŸ§ Datasets utilizados

| Clave  | Nombre del dataset            | Formato | DescripciÃ³n                                  |
| ------ | ----------------------------- | ------- | -------------------------------------------- |
| **D1** | `d1_songs_30000.csv`          | CSV     | 30,000 canciones con mÃ©tricas de Spotify     |
| **D2** | `d2_spotify_dashboard.xlsm`   | Excel   | Dataset limpio para anÃ¡lisis (formato Excel) |
| **D3** | `d3_spotify_artist_stats.csv` | CSV     | EstadÃ­sticas y metadatos de artistas         |

Coloca los tres archivos dentro de la carpeta:

```
data/raw/
```

---

## ğŸ§© EjecuciÃ³n del ETL (modo local)

1ï¸âƒ£ AsegÃºrate de que tu entorno virtual estÃ© activo.
2ï¸âƒ£ Ejecuta el script de prueba:

```bash
python tests/run_etl_local.py
```

Esto ejecutarÃ¡ automÃ¡ticamente las tres fases del pipeline:

* Limpieza y transformaciÃ³n de D1, D2 y D3 â†’ genera tres archivos en `data/interim/`
* UniÃ³n final (merge) â†’ genera `tracks_clean.csv` en `data/processed/`

**Resultado esperado:**

```
data/interim/d1_clean.csv
data/interim/d2_clean.csv
data/interim/d3_artists_clean.csv
data/processed/tracks_clean.csv
```

---

## ğŸŒ EjecuciÃ³n del API (modo FastAPI)

### 1ï¸âƒ£ Levantar el servidor

```bash
uvicorn api.main:app --reload
```

### 2ï¸âƒ£ Probar que la API funciona

Abre el navegador en:
ğŸ‘‰ [http://127.0.0.1:8000](http://127.0.0.1:8000)
DeberÃ­as ver:

```json
{"message": "API
```
